{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning with Attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OkliJqjiH-Uk",
        "colab_type": "code",
        "outputId": "c0ddbeb4-6bbc-405a-cec8-07eaa7dc6359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Hvass-Labs/TensorFlow-Tutorials.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlow-Tutorials'...\n",
            "remote: Enumerating objects: 459, done.\u001b[K\n",
            "remote: Total 459 (delta 0), reused 0 (delta 0), pack-reused 459\u001b[K\n",
            "Receiving objects: 100% (459/459), 61.89 MiB | 9.47 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rq3zbpkFHhYo",
        "colab_type": "code",
        "outputId": "a364644e-b40b-43cd-ffa4-60c68e4d6449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  TensorFlow-Tutorials\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Osx85BoIIdEC",
        "colab_type": "code",
        "outputId": "d17ad258-957b-44c1-e7cb-54301387cb20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd TensorFlow-Tutorials/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TensorFlow-Tutorials\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z4Lq1IK3Hach",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python coco.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIarY4yZIiCS",
        "colab_type": "code",
        "outputId": "e9ef0011-940e-4382-9884-8dc0c0b3ec8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01_Simple_Linear_Model.ipynb\t       22_Image_Captioning.ipynb\n",
            "02_Convolutional_Neural_Network.ipynb  23_Time-Series-Prediction.ipynb\n",
            "03B_Layers_API.ipynb\t\t       cache.py\n",
            "03C_Keras_API.ipynb\t\t       cifar10.py\n",
            "03_PrettyTensor.ipynb\t\t       coco.py\n",
            "04_Save_Restore.ipynb\t\t       convert.py\n",
            "05_Ensemble_Learning.ipynb\t       dataset.py\n",
            "06_CIFAR-10.ipynb\t\t       download.py\n",
            "07_Inception_Model.ipynb\t       europarl.py\n",
            "08_Transfer_Learning.ipynb\t       forks.md\n",
            "09_Video_Data.ipynb\t\t       images\n",
            "10_Fine-Tuning.ipynb\t\t       imdb.py\n",
            "11_Adversarial_Examples.ipynb\t       inception5h.py\n",
            "12_Adversarial_Noise_MNIST.ipynb       inception.py\n",
            "13B_Visual_Analysis_MNIST.ipynb        knifey.py\n",
            "13_Visual_Analysis.ipynb\t       LICENSE\n",
            "14_DeepDream.ipynb\t\t       mnist.py\n",
            "15_Style_Transfer.ipynb\t\t       __pycache__\n",
            "16_Reinforcement_Learning.ipynb        README.md\n",
            "17_Estimator_API.ipynb\t\t       reinforcement_learning.py\n",
            "18_TFRecords_Dataset_API.ipynb\t       requirements.txt\n",
            "19_Hyper-Parameters.ipynb\t       vgg16.py\n",
            "20_Natural_Language_Processing.ipynb   weather.py\n",
            "21_Machine_Translation.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZULd1stkIsuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from cache import cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2PJaxpiIxO6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tf.keras.models import Model  # This does not work!\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
        "from tensorflow.python.keras.applications import VGG16\n",
        "from tensorflow.python.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVryFdxBJLjB",
        "colab_type": "code",
        "outputId": "a362fef7-4b4d-4a9c-d69a-274fd6dc112a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "import coco\n",
        "coco.maybe_download_and_extract()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://images.cocodataset.org/zips/train2017.zip\n",
            "- Download progress: 98.5%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WynHPBA1JTFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_, filenames_train, captions_train = coco.load_records(train=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7vLNoxWeqgo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "num_images_train = len(filenames_train)\n",
        "num_images_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SE8g_lNeetf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_, filenames_val, captions_val = coco.load_records(train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vWudNni4ew5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image(path, size=None):\n",
        "    \"\"\"\n",
        "    Load the image from the given file-path and resize it\n",
        "    to the given size if not None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the image using PIL.\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # Resize image if desired.\n",
        "    if not size is None:\n",
        "        img = img.resize(size=size, resample=Image.LANCZOS)\n",
        "\n",
        "    # Convert image to numpy array.\n",
        "    img = np.array(img)\n",
        "\n",
        "    # Scale image-pixels so they fall between 0.0 and 1.0\n",
        "    img = img / 255.0\n",
        "\n",
        "    # Convert 2-dim gray-scale array to 3-dim RGB array.\n",
        "    if (len(img.shape) == 2):\n",
        "        img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3ZWCvoXezOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_image(idx, train):\n",
        "    \"\"\"\n",
        "    Load and plot an image from the training- or validation-set\n",
        "    with the given index.\n",
        "    \"\"\"\n",
        "\n",
        "    if train:\n",
        "        # Use an image from the training-set.\n",
        "        dir = coco.train_dir\n",
        "        filename = filenames_train[idx]\n",
        "        captions = captions_train[idx]\n",
        "    else:\n",
        "        # Use an image from the validation-set.\n",
        "        dir = coco.val_dir\n",
        "        filename = filenames_val[idx]\n",
        "        captions = captions_val[idx]\n",
        "\n",
        "    # Path for the image-file.\n",
        "    path = os.path.join(dir, filename)\n",
        "\n",
        "    # Print the captions for this image.\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "    \n",
        "    # Load the image and plot it.\n",
        "    img = load_image(path)\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxcxRt6Me2Jl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_image(idx=1, train=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p_1YDbNrfBVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Trasnfer Learning\n",
        "image_model = VGG16(include_top=True, weights='imagenet')\n",
        "image_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00_WRYiofLUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transfer_layer = image_model.get_layer('fc2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5iNlMwyPliVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_model_transfer = Model(inputs=image_model.input,\n",
        "                             outputs=transfer_layer.output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKKUCx_SlkOc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size = K.int_shape(image_model.input)[1:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3UWcinPlmXQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transfer_values_size = K.int_shape(transfer_layer.output)[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BW8kQrrploWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "We now make functions for processing all images in the data-set using the \n",
        "pre-trained image-model and saving the transfer-values in a cache-file so they can be reloaded quickly.\n",
        "'''\n",
        "\n",
        "def print_progress(count, max_count):\n",
        "    # Percentage completion.\n",
        "    pct_complete = count / max_count\n",
        "\n",
        "    # Status-message. Note the \\r which means the line should\n",
        "    # overwrite itself.\n",
        "    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n",
        "\n",
        "    # Print it.\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_GUCripYl06O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_images(data_dir, filenames, batch_size=32):\n",
        "    \"\"\"\n",
        "    Process all the given files in the given data_dir using the\n",
        "    pre-trained image-model and return their transfer-values.\n",
        "    \n",
        "    Note that we process the images in batches to save\n",
        "    memory and improve efficiency on the GPU.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Number of images to process.\n",
        "    num_images = len(filenames)\n",
        "\n",
        "    # Pre-allocate input-batch-array for images.\n",
        "    shape = (batch_size,) + img_size + (3,)\n",
        "    image_batch = np.zeros(shape=shape, dtype=np.float16)\n",
        "\n",
        "    # Pre-allocate output-array for transfer-values.\n",
        "    # Note that we use 16-bit floating-points to save memory.\n",
        "    shape = (num_images, transfer_values_size)\n",
        "    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n",
        "\n",
        "    # Initialize index into the filenames.\n",
        "    start_index = 0\n",
        "\n",
        "    # Process batches of image-files.\n",
        "    while start_index < num_images:\n",
        "        # Print the percentage-progress.\n",
        "        print_progress(count=start_index, max_count=num_images)\n",
        "\n",
        "        # End-index for this batch.\n",
        "        end_index = start_index + batch_size\n",
        "\n",
        "        # Ensure end-index is within bounds.\n",
        "        if end_index > num_images:\n",
        "            end_index = num_images\n",
        "\n",
        "        # The last batch may have a different batch-size.\n",
        "        current_batch_size = end_index - start_index\n",
        "\n",
        "        # Load all the images in the batch.\n",
        "        for i, filename in enumerate(filenames[start_index:end_index]):\n",
        "            # Path for the image-file.\n",
        "            path = os.path.join(data_dir, filename)\n",
        "\n",
        "            # Load and resize the image.\n",
        "            # This returns the image as a numpy-array.\n",
        "            img = load_image(path, size=img_size)\n",
        "\n",
        "            # Save the image for later use.\n",
        "            image_batch[i] = img\n",
        "\n",
        "        # Use the pre-trained image-model to process the image.\n",
        "        # Note that the last batch may have a different size,\n",
        "        # so we only use the relevant images.\n",
        "        transfer_values_batch = \\\n",
        "            image_model_transfer.predict(image_batch[0:current_batch_size])\n",
        "\n",
        "        # Save the transfer-values in the pre-allocated array.\n",
        "        transfer_values[start_index:end_index] = \\\n",
        "            transfer_values_batch[0:current_batch_size]\n",
        "\n",
        "        # Increase the index for the next loop-iteration.\n",
        "        start_index = end_index\n",
        "\n",
        "    # Print newline.\n",
        "    print()\n",
        "\n",
        "    return transfer_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcga9qwMl50P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_images_train():\n",
        "    print(\"Processing {0} images in training-set ...\".format(len(filenames_train)))\n",
        "\n",
        "    # Path for the cache-file.\n",
        "    cache_path = os.path.join(coco.data_dir,\n",
        "                              \"transfer_values_train.pkl\")\n",
        "\n",
        "    # If the cache-file already exists then reload it,\n",
        "    # otherwise process all images and save their transfer-values\n",
        "    # to the cache-file so it can be reloaded quickly.\n",
        "    transfer_values = cache(cache_path=cache_path,\n",
        "                            fn=process_images,\n",
        "                            data_dir=coco.train_dir,\n",
        "                            filenames=filenames_train)\n",
        "\n",
        "    return transfer_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h9ouhAFul8RY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_images_val():\n",
        "    print(\"Processing {0} images in validation-set ...\".format(len(filenames_val)))\n",
        "\n",
        "    # Path for the cache-file.\n",
        "    cache_path = os.path.join(coco.data_dir, \"transfer_values_val.pkl\")\n",
        "\n",
        "    # If the cache-file already exists then reload it,\n",
        "    # otherwise process all images and save their transfer-values\n",
        "    # to the cache-file so it can be reloaded quickly.\n",
        "    transfer_values = cache(cache_path=cache_path,\n",
        "                            fn=process_images,\n",
        "                            data_dir=coco.val_dir,\n",
        "                            filenames=filenames_val)\n",
        "\n",
        "    return transfer_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8Xi9zEEl-cd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import sys\n",
        "transfer_values_train = process_images_train()\n",
        "print(\"dtype:\", transfer_values_train.dtype)\n",
        "print(\"shape:\", transfer_values_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ocio9_h9mBbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "\n",
        "mark_start = 'ssss '\n",
        "mark_end = ' eeee'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WIwP4Gg34ADJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mark_captions(captions_listlist):\n",
        "    captions_marked = [[mark_start + caption + mark_end\n",
        "                        for caption in captions_list]\n",
        "                        for captions_list in captions_listlist]\n",
        "    \n",
        "    return captions_marked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6Fuu1s54Cnp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "captions_train_marked = mark_captions(captions_train)\n",
        "captions_train_marked[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xh8LDbF4E5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "captions_train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRFoZLNz4Iiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def flatten(captions_listlist):\n",
        "    captions_list = [caption\n",
        "                     for captions_list in captions_listlist\n",
        "                     for caption in captions_list]\n",
        "    \n",
        "    return captions_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xvXYpQnO4L4g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "captions_train_flat = flatten(captions_train_marked)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jk7RS9wI4N1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_words = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilCx4CPb4Q12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TokenizerWrap(Tokenizer):\n",
        "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, num_words=None):\n",
        "        \"\"\"\n",
        "        :param texts: List of strings with the data-set.\n",
        "        :param num_words: Max number of words to use.\n",
        "        \"\"\"\n",
        "\n",
        "        Tokenizer.__init__(self, num_words=num_words)\n",
        "\n",
        "        # Create the vocabulary from the texts.\n",
        "        self.fit_on_texts(texts)\n",
        "\n",
        "        # Create inverse lookup from integer-tokens to words.\n",
        "        self.index_to_word = dict(zip(self.word_index.values(),\n",
        "                                      self.word_index.keys()))\n",
        "\n",
        "    def token_to_word(self, token):\n",
        "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
        "\n",
        "        word = \" \" if token == 0 else self.index_to_word[token]\n",
        "        return word \n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
        "\n",
        "        # Create a list of the individual words.\n",
        "        words = [self.index_to_word[token]\n",
        "                 for token in tokens\n",
        "                 if token != 0]\n",
        "        \n",
        "        # Concatenate the words to a single string\n",
        "        # with space between all the words.\n",
        "        text = \" \".join(words)\n",
        "\n",
        "        return text\n",
        "    \n",
        "    def captions_to_tokens(self, captions_listlist):\n",
        "        \"\"\"\n",
        "        Convert a list-of-list with text-captions to\n",
        "        a list-of-list of integer-tokens.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Note that text_to_sequences() takes a list of texts.\n",
        "        tokens = [self.texts_to_sequences(captions_list)\n",
        "                  for captions_list in captions_listlist]\n",
        "        \n",
        "        return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o400NX-l4UJ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "tokenizer = TokenizerWrap(texts=captions_train_flat,\n",
        "                          num_words=num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IufIYf6U4Xxh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "token_start = tokenizer.word_index[mark_start.strip()]\n",
        "token_end = tokenizer.word_index[mark_end.strip()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pysdVHXN4ge_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokens_train = tokenizer.captions_to_tokens(captions_train_marked)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CqVurLsd4ijM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokens_train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltAaKlt64k90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "captions_train_marked[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_MkuFsaC44qQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_random_caption_tokens(idx):\n",
        "    \"\"\"\n",
        "    Given a list of indices for images in the training-set,\n",
        "    select a token-sequence for a random caption,\n",
        "    and return a list of all these token-sequences.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize an empty list for the results.\n",
        "    result = []\n",
        "\n",
        "    # For each of the indices.\n",
        "    for i in idx:\n",
        "        # The index i points to an image in the training-set.\n",
        "        # Each image in the training-set has at least 5 captions\n",
        "        # which have been converted to tokens in tokens_train.\n",
        "        # We want to select one of these token-sequences at random.\n",
        "\n",
        "        # Get a random index for a token-sequence.\n",
        "        j = np.random.choice(len(tokens_train[i]))\n",
        "\n",
        "        # Get the j'th token-sequence for image i.\n",
        "        tokens = tokens_train[i][j]\n",
        "\n",
        "        # Add this token-sequence to the list of results.\n",
        "        result.append(tokens)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OxzvGzAx47m3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_generator(batch_size):\n",
        "    \"\"\"\n",
        "    Generator function for creating random batches of training-data.\n",
        "    \n",
        "    Note that it selects the data completely randomly for each\n",
        "    batch, corresponding to sampling of the training-set with\n",
        "    replacement. This means it is possible to sample the same\n",
        "    data multiple times within a single epoch - and it is also\n",
        "    possible that some data is not sampled at all within an epoch.\n",
        "    However, all the data should be unique within a single batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Infinite loop.\n",
        "    while True:\n",
        "        # Get a list of random indices for images in the training-set.\n",
        "        idx = np.random.randint(num_images_train,\n",
        "                                size=batch_size)\n",
        "        \n",
        "        # Get the pre-computed transfer-values for those images.\n",
        "        # These are the outputs of the pre-trained image-model.\n",
        "        transfer_values = transfer_values_train[idx]\n",
        "\n",
        "        # For each of the randomly chosen images there are\n",
        "        # at least 5 captions describing the contents of the image.\n",
        "        # Select one of those captions at random and get the\n",
        "        # associated sequence of integer-tokens.\n",
        "        tokens = get_random_caption_tokens(idx)\n",
        "\n",
        "        # Count the number of tokens in all these token-sequences.\n",
        "        num_tokens = [len(t) for t in tokens]\n",
        "        \n",
        "        # Max number of tokens.\n",
        "        max_tokens = np.max(num_tokens)\n",
        "        \n",
        "        # Pad all the other token-sequences with zeros\n",
        "        # so they all have the same length and can be\n",
        "        # input to the neural network as a numpy array.\n",
        "        tokens_padded = pad_sequences(tokens,\n",
        "                                      maxlen=max_tokens,\n",
        "                                      padding='post',\n",
        "                                      truncating='post')\n",
        "        \n",
        "        # Further prepare the token-sequences.\n",
        "        # The decoder-part of the neural network\n",
        "        # will try to map the token-sequences to\n",
        "        # themselves shifted one time-step.\n",
        "        decoder_input_data = tokens_padded[:, 0:-1]\n",
        "        decoder_output_data = tokens_padded[:, 1:]\n",
        "\n",
        "        # Dict for the input-data. Because we have\n",
        "        # several inputs, we use a named dict to\n",
        "        # ensure that the data is assigned correctly.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'decoder_input': decoder_input_data,\n",
        "            'transfer_values_input': transfer_values\n",
        "        }\n",
        "\n",
        "        # Dict for the output-data.\n",
        "        y_data = \\\n",
        "        {\n",
        "            'decoder_output': decoder_output_data\n",
        "        }\n",
        "        \n",
        "        yield (x_data, y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B0vvZ3C05gv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "generator = batch_generator(batch_size=batch_size)\n",
        "batch = next(generator)\n",
        "batch_x = batch[0]\n",
        "batch_y = batch[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uo1nh7lw5tSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_captions_train = [len(captions) for captions in captions_train]\n",
        "total_num_captions_train = np.sum(num_captions_train)\n",
        "steps_per_epoch = int(total_num_captions_train / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iR0n0PSR51mi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state_size = 512\n",
        "embedding_size = 128\n",
        "transfer_values_input = Input(shape=(transfer_values_size,),\n",
        "                              name='transfer_values_input')\n",
        "\n",
        "decoder_transfer_map = Dense(state_size,\n",
        "                             activation='tanh',\n",
        "                             name='decoder_transfer_map')\n",
        "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
        "decoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='decoder_embedding')\n",
        "\n",
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                   return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True)\n",
        "\n",
        "\n",
        "decoder_dense = Dense(num_words,\n",
        "                      activation='linear',\n",
        "                      name='decoder_output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ys3uvg16VgW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def connect_decoder(transfer_values):\n",
        "    # Map the transfer-values so the dimensionality matches\n",
        "    # the internal state of the GRU layers. This means\n",
        "    # we can use the mapped transfer-values as the initial state\n",
        "    # of the GRU layers.\n",
        "    initial_state = decoder_transfer_map(transfer_values)\n",
        "\n",
        "    # Start the decoder-network with its input-layer.\n",
        "    net = decoder_input\n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = decoder_embedding(net)\n",
        "    \n",
        "    # Connect all the GRU layers.\n",
        "    net = decoder_gru1(net, initial_state=initial_state)\n",
        "    net = decoder_gru2(net, initial_state=initial_state)\n",
        "    net = decoder_gru3(net, initial_state=initial_state)\n",
        "\n",
        "    # Connect the final dense layer that converts to\n",
        "    # one-hot encoded arrays.\n",
        "    decoder_output = decoder_dense(net)\n",
        "    \n",
        "    return decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOPYa_ER6Xft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "decoder_output = connect_decoder(transfer_values=transfer_values_input)\n",
        "\n",
        "decoder_model = Model(inputs=[transfer_values_input, decoder_input],\n",
        "                      outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4E2GOVk_6ZhF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the cross-entropy loss between y_true and y_pred.\n",
        "    \n",
        "    y_true is a 2-rank tensor with the desired output.\n",
        "    The shape is [batch_size, sequence_length] and it\n",
        "    contains sequences of integer-tokens.\n",
        "\n",
        "    y_pred is the decoder's output which is a 3-rank tensor\n",
        "    with shape [batch_size, sequence_length, num_words]\n",
        "    so that for each sequence in the batch there is a one-hot\n",
        "    encoded array of length num_words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the loss. This outputs a\n",
        "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                          logits=y_pred)\n",
        "\n",
        "    # Keras may reduce this across the first axis (the batch)\n",
        "    # but the semantics are unclear, so to be sure we use\n",
        "    # the loss across the entire 2-rank tensor, we reduce it\n",
        "    # to a single scalar with the mean function.\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "\n",
        "    return loss_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pWOL0Tqc6cmt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = Adam(lr=1e-3)\n",
        "\n",
        "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n",
        "\n",
        "decoder_model.compile(optimizer=optimizer,\n",
        "                      loss=sparse_cross_entropy,\n",
        "                      target_tensors=[decoder_target])\n",
        "\n",
        "path_checkpoint = '22_checkpoint.keras'\n",
        "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
        "                                      verbose=1,\n",
        "                                      save_weights_only=True)\n",
        "\n",
        "callback_tensorboard = TensorBoard(log_dir='./22_logs/',\n",
        "                                   histogram_freq=0,\n",
        "                                   write_graph=False)\n",
        "\n",
        "callbacks = [callback_checkpoint, callback_tensorboard]\n",
        "\n",
        "# try:\n",
        "#     decoder_model.load_weights(path_checkpoint)\n",
        "# except Exception as error:\n",
        "#     print(\"Error trying to load checkpoint.\")\n",
        "#     print(error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y8V3VAeg6tP7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "decoder_model.fit_generator(generator=generator,\n",
        "                            steps_per_epoch=steps_per_epoch,\n",
        "                            epochs=6,\n",
        "                            callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WOrBmTbaETpj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#     decoder_model.load_weights(path_checkpoint)\n",
        "# except Exception as error:\n",
        "#     print(\"Error trying to load checkpoint.\")\n",
        "#     print(error)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('22_checkpoint.keras')\n",
        "# files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2OsDilgI48wU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "snjCyOyY6y47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_caption(image_path, max_tokens=30):\n",
        "    \"\"\"\n",
        "    Generate a caption for the image in the given path.\n",
        "    The caption is limited to the given number of tokens (words).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and resize the image.\n",
        "    image = load_image(image_path, size=img_size)\n",
        "    \n",
        "    # Expand the 3-dim numpy array to 4-dim\n",
        "    # because the image-model expects a whole batch as input,\n",
        "    # so we give it a batch with just one image.\n",
        "    image_batch = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # Process the image with the pre-trained image-model\n",
        "    # to get the transfer-values.\n",
        "    transfer_values = image_model_transfer.predict(image_batch)\n",
        "\n",
        "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
        "    # This holds just a single sequence of integer-tokens,\n",
        "    # but the decoder-model expects a batch of sequences.\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "    # The first input-token is the special start-token for 'ssss '.\n",
        "    token_int = token_start\n",
        "\n",
        "    # Initialize an empty output-text.\n",
        "    output_text = ''\n",
        "\n",
        "    # Initialize the number of tokens we have processed.\n",
        "    count_tokens = 0\n",
        "\n",
        "    # While we haven't sampled the special end-token for ' eeee'\n",
        "    # and we haven't processed the max number of tokens.\n",
        "    while token_int != token_end and count_tokens < max_tokens:\n",
        "        # Update the input-sequence to the decoder\n",
        "        # with the last token that was sampled.\n",
        "        # In the first iteration this will set the\n",
        "        # first element to the start-token.\n",
        "        decoder_input_data[0, count_tokens] = token_int\n",
        "\n",
        "        # Wrap the input-data in a dict for clarity and safety,\n",
        "        # so we are sure we input the data in the right order.\n",
        "        x_data = \\\n",
        "        {\n",
        "            'transfer_values_input': transfer_values,\n",
        "            'decoder_input': decoder_input_data\n",
        "        }\n",
        "\n",
        "        # Note that we input the entire sequence of tokens\n",
        "        # to the decoder. This wastes a lot of computation\n",
        "        # because we are only interested in the last input\n",
        "        # and output. We could modify the code to return\n",
        "        # the GRU-states when calling predict() and then\n",
        "        # feeding these GRU-states as well the next time\n",
        "        # we call predict(), but it would make the code\n",
        "        # much more complicated.\n",
        "        \n",
        "        # Input this data to the decoder and get the predicted output.\n",
        "        decoder_output = decoder_model.predict(x_data)\n",
        "\n",
        "        # Get the last predicted token as a one-hot encoded array.\n",
        "        # Note that this is not limited by softmax, but we just\n",
        "        # need the index of the largest element so it doesn't matter.\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "\n",
        "        # Convert to an integer-token.\n",
        "        token_int = np.argmax(token_onehot)\n",
        "\n",
        "        # Lookup the word corresponding to this integer-token.\n",
        "        sampled_word = tokenizer.token_to_word(token_int)\n",
        "\n",
        "        # Append the word to the output-text.\n",
        "        output_text += \" \" + sampled_word\n",
        "\n",
        "        # Increment the token-counter.\n",
        "        count_tokens += 1\n",
        "\n",
        "    # This is the sequence of tokens output by the decoder.\n",
        "    output_tokens = decoder_input_data[0]\n",
        "\n",
        "    # Plot the image.\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "    \n",
        "    # Print the predicted caption.\n",
        "    print(\"Predicted caption:\")\n",
        "    print(output_text)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pDJEBolqYHqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_caption(\"images/parrot_cropped1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aDoAOtJCZt70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_caption(\"images/elon_musk.jpg\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hpkYtfLdZuoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_caption_coco(idx, train=False):\n",
        "    \"\"\"\n",
        "    Generate a caption for an image in the COCO data-set.\n",
        "    Use the image with the given index in either the\n",
        "    training-set (train=True) or validation-set (train=False).\n",
        "    \"\"\"\n",
        "    \n",
        "    if train:\n",
        "        # Use image and captions from the training-set.\n",
        "        data_dir = coco.train_dir\n",
        "        filename = filenames_train[idx]\n",
        "        captions = captions_train[idx]\n",
        "    else:\n",
        "        # Use image and captions from the validation-set.\n",
        "        data_dir = coco.val_dir\n",
        "        filename = filenames_val[idx]\n",
        "        captions = captions_val[idx]\n",
        "\n",
        "    # Path for the image-file.\n",
        "    path = os.path.join(data_dir, filename)\n",
        "\n",
        "    # Use the model to generate a caption of the image.\n",
        "    generate_caption(image_path=path)\n",
        "\n",
        "    # Print the true captions from the data-set.\n",
        "    print(\"True captions:\")\n",
        "    for caption in captions:\n",
        "        print(caption)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LhzULhGDZ8Y-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_caption_coco(idx=1\n",
        "       , train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E5BFqVFrZ9B8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}